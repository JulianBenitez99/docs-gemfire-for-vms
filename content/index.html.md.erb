---
title: Pivotal Cloud Cache
owner: Cloud Cache Engineers
---

## <a id='overview'></a>Overview

Pivotal Cloud Cache (PCC) is a high-performance, highly available caching layer for Pivotal Cloud Foundry (PCF).
PCC offers an in-memory key-value store.
The product delivers low-latency responses to a large number of concurrent data access requests.

PCC provides a service broker for on-demand creation of in-memory data clusters that are dedicated to the PCF space and tuned for the intended use cases defined by the plan.
Service operators can create multiple plans to support different use cases.
PCC uses Pivotal GemFire.

You can use PCC to store any kind of data objects using the [Pivotal GemFire Java client library](http://gemfire-90-javadocs.docs.pivotal.io/).

This documentation performs the following functions:

- Describes the features and architecture of PCC
- Provides the PCF operator with instructions for installing, configuring, and maintaining PCC
- Provides app developers instructions for choosing a service plan, creating and deleting PCC service instances, and binding apps

## <a id='snapshot'></a> Product Snapshot

The following table provides version and version-support information about Pivotal Cloud Cache:

<table class="nice">
    <th>Element</th>
    <th>Details</th>
    <tr>
        <td>Version</td>
        <td>v1.1.0</td>
    </tr>
    <tr>
        <td>Release date</td>
        <td>July 21, 2017</td>
    </tr>
    <tr>
        <td>Software component version</td>
        <td>GemFire v9.0.4</td>
    </tr>
    <tr>
        <td>Compatible Ops Manager version(s)</td>
        <td>v1.10.x and v1.11.x</td>
    </tr>
    <tr>
        <td>Compatible Elastic Runtime version(s)</td>
        <td>v1.10.x and later, v1.11.x</td>
    </tr>
    <tr>
        <td>IaaS support</td>
        <td>AWS, Azure, GCP, OpenStack, and vSphere</td>
    </tr>
    <tr>
        <td>IPsec support</td>
        <td>No</td>
    </tr>
</table>

## <a id='known-issues'></a>Known Issues

### Service Instance Upgrade Fails When Pulse has Open Connections

**Affected Versions**: 1.0.0 - 1.0.5

The `upgrade-all-service-instances` errand fails. On the Cloud Cache service instance deployment, this may appear with an error similar to the following:

```
 19:38:27 | Updating instance locator: locator/d6d98feb-f005-49ca-b1c1-3e2803811cc8 (0) (canary) (00:10:20)
            L Error: Action Failed get_task: Task f425d048-954e-424c-642f-3ba2f2b3bec4 result: Unmounting persistent disk: Running command: 'umount /dev/sdc1', stdout: '', stderr: 'umount: /var/vcap/store: device is busy.
        (In some cases useful info about processes that use
         the device is found by lsof(8) or fuser(1))
         ': exit status 1
```

This is due to a bug in GemFire where the locators cannot stop successfully when there is an open connection to the Pulse dashboard. This issue can be tracked in the Apache Geode Jira [here](https://issues.apache.org/jira/browse/GEODE-3191).

##### Mitigation Option 1

If possible, close all connections to the Pulse dashboards before upgrading the Pivotal Cloud Cache tile.

##### Mitigation Option 2

If you have not yet run the `upgrade-all-service-instances` errand, follow the steps below:

1. SSH onto the Operations Manager VM.
1. Identify each Cloud Cache deployment using the BOSH CLI.
1. For each Cloud Cache deployment, use `bosh ssh` to ssh onto a locator VM.
1. On the locator VM, change to be the root user by running `sudo su`.
1. On the locator VM, use `monit` to stop the `route_registrar` job by running `monit stop route_registrar`.
1. Repeat steps 4-5 for all the locators for each Cloud Cache deployment.
1. Navigate back to the Operations Manager dashboard and click **Apply Changes**.
1. The `upgrade-all-service-instances` errand should complete successfully.

##### Mitigation Option 3

If you have already run the `upgrade-all-service-instances` errand and now have a stopped locator in one of your Cloud Cache deployments, follow the steps below:

1. SSH onto the Operations Manager VM.
1. Identify the locator which is in a stopped state using the BOSH CLI. This can be done by running `bosh vms`.
1. On the locator VM, change yourself to the root user by running `sudo su`.
1. (Optional) On the locator VM, run `monit summary` to confirm that the `gemfire-locator` process is not monitored.
1. Kill the process by running `kill -9 "$(ps -ef | grep LocatorLauncher | head -n 1 | awk '{print $2}')"`.

After killing the LocatorLauncher process on the stopped Locator VM, proceed with option 2 above.

### Locator Missing From Cluster

**Affected Versions**: 1.0.0 - 1.0.5

A locator VM might fail to listen on its port and the status not be reported by BOSH.
  You only know that the locator VM has gone down when you run the `list members` command in gfsh.

#### Mitigations

SSH into the missing locator VM and run the `monit restart` command.

### Pulse Monitoring Tool Issue
The topology diagram in the Pulse monitoring tool might not be accurate, and might display more members in a cluster than the cluster actually contains. 
However the numerical value for members, shown in the top bar of the tool, is accurate.

## <a id='release-notes'></a>Release Notes
### <a id='v1.1.1'></a>Pivotal Cloud Cache v1.1.1

#### <a id='new-in-this-release'></a>New in This Release
Version 1.1.1 of PCC includes the following bug fixes:

 - Improved password requirement of the testing utility 
 - Increased `monit` for smoke tests to be successful
 - Improved kill command to force kill in the event of `gfsh stop` failure
 - Fixed upgrading a cloud cache service failure when `syslog tls` is disabled and `Send service instance logs to external syslog` is enabled

### <a id='v1.1.0'></a>Pivotal Cloud Cache v1.1.0

#### <a id='new-in-this-release'></a>New in This Release

Version 1.1 of PCC includes the following new features:

- HTTP session caching is supported in PCC.
- PCC is compatible with PCF v1.10 and v1.11.
- PCC signs BOSH deployments with SHA-2 to prevent certificate collisions.
- Syslog setup supports standard RFC format.

### <a id="16x"></a>Release Notes for Earlier Versions

For v1.0.x versions of PCC, see [Release Notes](http://docs.pivotal.io/p-cloud-cache/1-0/index.html#release-notes) in the v1.0 version of this documentation.

## <a id='architecture'></a>Architecture

PCC deploys cache clusters that use Pivotal GemFire to provide high availability, replication guarantees, and eventual consistency.

When you first spin up a cluster, you will have three locators and at least four servers.

<% mermaid_diagram do %>
  graph TD;
  Client
  subgraph P-CloudCache Cluster
  subgraph locators
  Locator1
  Locator2
  Locator3
  end
  subgraph servers
  Server1
  Server2
  Server3
  Server4
  end
  end
  Client==>Locator1
  Client-->Server1
  Client-->Server2
  Client-->Server3
  Client-->Server4
<% end %>


If you scale the cluster up, you will have more servers, increasing the capacity of the cache. There always will be three locators.

<% mermaid_diagram do %>
  graph TD;
  Client
  subgraph P-CloudCache Cluster
  subgraph locators
  Locator1
  Locator2
  Locator3
  end
  subgraph servers
  Server1
  Server2
  Server3
  Server4
  Server5
  Server6
  Server7
  end
  end
  Client==>Locator1
  Client-->Server1
  Client-->Server2
  Client-->Server3
  Client-->Server4
  Client-->Server5
  Client-->Server6
  Client-->Server7
<% end %>


When a client connects to the cluster, it first connects to a locator. The locator replies with the IP address of a server for it to talk to. The client then connects to that server.

<% mermaid_diagram do %>
  sequenceDiagram
    participant Client
    participant Locator
    participant Server1
    Client->>+Locator: What servers can I talk to?
    Locator->>-Client: Server1
    Client->>Server1: Hello!
<% end %>

When the client wants to read or write data, it sends a request directly to the server.

<% mermaid_diagram do %>
  sequenceDiagram
    participant Client
    participant Server1
    Client->>+Server1: What's the value for KEY?
    Server1->>-Client: VALUE
<% end %>


If the server doesn't have the data locally, it fetches it from another server.

<% mermaid_diagram do %>
  sequenceDiagram
    participant Client
    participant Server1
    participant Server2
    Client->>+Server1: What's the value for KEY?
    Server1->>+Server2: What's the value for KEY?
    Server2->>-Server1: VALUE
    Server1->>-Client: VALUE
<% end %>

## <a id='workflow'></a>Workflow

The workflow for the PCF admin setting up a PCC service plan:

<% mermaid_diagram do %>
  graph TD;
  subgraph PCF Admin Actions
  s1
  s2
  end

  subgraph Developer Actions
  s4
  end

  s1[1. Upload P-CloudCache.pivotal to Ops Manager]
  s2[2. Configure CloudCache Service Plans, i.e. caching-small]
  s1-->s2
  s3[3. Ops Manager deploys CloudCache Service Broker]
  s2-->s3
  s4[4. Developer calls `cf create-service p-cloudcache caching-small test`]
  s3-->s4
  s5[5. Ops Manager creates a CloudCache cluster following the caching-small specifications]
  s4-->s5
<% end %>

## <a id='recommended-usage'></a>Recommended Usage and Limitations

- PCC for PCF can be used as a cache. It supports the [look-aside cache pattern](https://content.pivotal.io/blog/an-introduction-to-look-aside-vs-inline-caching-patterns).
- PCC can be used to store objects in key/value format, where value can be any object.
- PCC works with gfsh versions v9.0.0 and later
- Any gfsh command not explained in the PCC documentation is **not supported**.
- PCC supports basic OQL queries, with no support for joins.

### Limitations

- Scale down of the cluster is not supported.
- Plan migrations, for example, `-p` flag with the `cf update-service` command, are not supported.
- WAN (Cross Data Center) replication is not supported.
- Persistent regions are not supported.

## <a id='security'></a>Security

Pivotal recommends that you do the following:

- Run PCC for PCF in its own network
- Use a load balancer to block direct, outside access to the Gorouter

To allow PCC network access from apps, you must create application security groups that allow access on the following ports: 

* 1099
* 8080
* 40404
* 55221

For more information, see the PCF [Application Security Groups](https://docs.pivotal.io/pivotalcf/adminguide/app-sec-groups.html#creating-groups) topic.

### Authentication

Clusters are created with two default users: `cluster_operator` and `developer`.
A cluster can only be accessed using one of these two users.
All client applications, gfsh, and JMX clients must authenticate as one of these users accounts to access the cluster.

### Authorization

Default user roles `cluster_operator` and `developer` have different permissions:

- `cluster_operator` role has DATA:MANAGE, DATA:WRITE, and DATA:READ permissions.
- `developer` role has DATA:WRITE and DATA:READ permissions.

You can find more details about these permissions in the Pivotal GemFire [Implementing Authorization](http://gemfire.docs.pivotal.io/geode/managing/security/implementing_authorization.html) topic.

## <a id='feedback'></a> Feedback
Please provide any bugs, feature requests, or questions to the [Pivotal Cloud Foundry Feedback list](mailto:pivotal–cf–feedback@pivotal.io).
